{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import rel_entr, entr, xlogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available dataset\n",
    "datasets = [\"DTD\",\"EuroSAT\",\"GTSRB\",\"MNIST\",\"SVHN\",\"Caltech256\",\"SUN397\",\"iWildCam\",\"iWildCamID\",\"CINIC10\",\"CIFAR10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 1e-5\n",
    "ub = 100\n",
    "lap_eps = 1e-5\n",
    "def laplace_pred_mat(P):\n",
    "    P+=lap_eps\n",
    "    row_sums = P.sum(axis=1)\n",
    "    normalized_P = P.div(row_sums, axis=0)\n",
    "    return normalized_P\n",
    "    \n",
    "def calc_kl(P, Q):\n",
    "    P = laplace_pred_mat(P)\n",
    "    Q = laplace_pred_mat(Q)\n",
    "    return np.sum(rel_entr(P, Q).clip(lb,ub), axis=1).clip(lb,ub)\n",
    "\n",
    "# Calculate Entropy\n",
    "def calc_entropy(P):\n",
    "    P = laplace_pred_mat(P)\n",
    "    return np.sum(entr(P), axis=1).clip(lb,ub)\n",
    "\n",
    "# Calculate Cross-Entropy\n",
    "def calc_cross_ent(P, Q):\n",
    "    P = laplace_pred_mat(P)\n",
    "    Q = laplace_pred_mat(Q)\n",
    "    return (-np.sum(xlogy(P,Q), axis=1)).clip(lb,ub)\n",
    "\n",
    "def calc_nll(p):\n",
    "    P = laplace_pred_mat(P)\n",
    "    return -np.log(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for ds in datasets:\n",
    "    df18 = pd.read_csv(f\"../Pred/{ds}_Resnet18.csv\")\n",
    "    df50 = pd.read_csv(f\"../Pred/{ds}_Resnet50.csv\")\n",
    "    df101 = pd.read_csv(f\"../Pred/{ds}_Resnet101.csv\")\n",
    "    label = pd.read_csv(f\"../Pred/target_{ds}.csv\")\n",
    "    prediction = pd.DataFrame({\n",
    "    \"pred18\": df18.idxmax(axis=1),\n",
    "    \"pred50\": df50.idxmax(axis=1),\n",
    "    \"pred101\": df101.idxmax(axis=1),\n",
    "    \"target\": label['target']\n",
    "    })\n",
    "    prediction[\"pred18\"] = prediction[\"pred18\"].str.extract('(\\d+)').astype(int)\n",
    "    prediction[\"pred50\"] = prediction[\"pred50\"].str.extract('(\\d+)').astype(int)\n",
    "    prediction[\"pred101\"] = prediction[\"pred101\"].str.extract('(\\d+)').astype(int)\n",
    "    prediction.to_csv(f\"csv/prediction_{ds}.csv\")\n",
    "    unc_pred = pd.DataFrame()\n",
    "    unc_pred[\"kl_18_101\"] = calc_kl(df18,df101)\n",
    "    unc_pred[\"kl_101_18\"] = calc_kl(df101,df18)\n",
    "    unc_pred[\"kl_18_50\"] = calc_kl(df18,df50)\n",
    "    unc_pred[\"kl_50_18\"] = calc_kl(df50,df18)\n",
    "    unc_pred[\"ent_18\"] = calc_entropy(df18)\n",
    "    unc_pred[\"ent_50\"] = calc_entropy(df50)\n",
    "    unc_pred[\"ent_101\"] = calc_entropy(df101)\n",
    "    unc_pred[\"ce_18_101\"] = calc_cross_ent(df18,df101)\n",
    "    unc_pred[\"ce_101_18\"] = calc_cross_ent(df101,df18)\n",
    "    unc_pred[\"ce_18_50\"] = calc_cross_ent(df18,df50)\n",
    "    unc_pred[\"ce_50_18\"] = calc_cross_ent(df50,df18)\n",
    "    # Calculate NLL and Brier Scores for each sample and add as columns\n",
    "    # ==============\n",
    "    flat_label = label['target'].values.flatten()\n",
    "    df18_array = df18.values\n",
    "    df50_array = df50.values\n",
    "    df101_array = df101.values\n",
    "    # print(\"df18_array shape:\", df18_array.shape)\n",
    "    # print(\"df101_array shape:\", df101_array.shape)\n",
    "    # print(\"flat_label shape:\", flat_label.shape)\n",
    "    # Extracting the predicted probabilities for the true class labels\n",
    "    predicted_probabilities18 = df18_array[np.arange(df18_array.shape[0]), flat_label]\n",
    "    predicted_probabilities50 = df50_array[np.arange(df50_array.shape[0]), flat_label]\n",
    "    predicted_probabilities101 = df101_array[np.arange(df101_array.shape[0]), flat_label]\n",
    "    # NLL\n",
    "    unc_pred[\"nll_18\"] = (-np.log(predicted_probabilities18)).clip(0)\n",
    "    unc_pred[\"nll_50\"] = (-np.log(predicted_probabilities50)).clip(0)\n",
    "    unc_pred[\"nll_101\"] = (-np.log(predicted_probabilities101)).clip(0)\n",
    "    # Brier Scores:\n",
    "    unc_pred[\"brier_18\"] = (predicted_probabilities18-1)**2 + np.sum(np.square(df18),axis=1) - predicted_probabilities18**2\n",
    "    unc_pred[\"brier_50\"] = (predicted_probabilities50-1)**2 + np.sum(np.square(df50),axis=1) - predicted_probabilities50**2\n",
    "    unc_pred[\"brier_101\"] = (predicted_probabilities101-1)**2 + np.sum(np.square(df101),axis=1) - predicted_probabilities101**2\n",
    "    \n",
    "    unc_pred[\"softmax_response18\"] = np.max(df18_array,axis=1)\n",
    "    unc_pred[\"softmax_response50\"] = np.max(df50_array,axis=1)\n",
    "    unc_pred[\"softmax_response101\"] = np.max(df101_array,axis=1)\n",
    "    \n",
    "    unc_pred.to_csv(f\"csv/uncertainty_{ds}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc340",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
